{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778ac82e-a167-4208-bb4b-06a506b60ae0",
   "metadata": {},
   "source": [
    "# Building a PySpark-Powered LLM Data Processing System\n",
    "This guide will help me (2tu/SKS) to create a project that combines PySpark for distributed data processing with local LLM (Llama) and OpenAI API capabilities, and subsequently compare them. Here's a comprehensive plan:\n",
    "\n",
    "Project Architecture Overview\n",
    "Data Ingestion Layer: PySpark for loading and preprocessing large datasets\n",
    "\n",
    "Processing Layer: Distributed NLP tasks using PySpark\n",
    "\n",
    "LLM Integration: Local Llama model and OpenAI API for advanced text processing\n",
    "\n",
    "Output/Storage: Processed results stored efficiently\n",
    "\n",
    "Step 1: Setting Up environment prerequisites:\n",
    "\n",
    "Java 8/11: installed\n",
    "\n",
    "Python 3.7+: installed\n",
    "\n",
    "Apache Spark: installed\n",
    "\n",
    "Jupyter Notebook: Installed and used since high school.\n",
    "\n",
    "Llama model: downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93524478-0f89-4812-8909-632f47757e04",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07eb972-1b1e-4d1f-b985-c95e0275cad7",
   "metadata": {},
   "source": [
    "# Create virtual environment\n",
    "python -m venv llm-spark-env\n",
    "source llm-spark-env/bin/activate  # Linux/Mac\n",
    "# llm-spark-env\\Scripts\\activate  # Windows\n",
    "\n",
    "# Install required packages\n",
    "pip install pyspark jupyter openai transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d54353-6a1a-4696-99b5-e6da7936acc5",
   "metadata": {},
   "source": [
    "# Step 2: PySpark Initialization\n",
    "Create a utils.py file for Spark session management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9907c7-a2fc-4a04-b898-2d412129c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def create_spark_session(app_name=\"LLM-Spark-Processing\"):\n",
    "    conf = SparkConf() \\\n",
    "        .setAppName(app_name) \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "def stop_spark_session(spark):\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508c2cf-58fb-472e-b841-e294168c180d",
   "metadata": {},
   "source": [
    "# Step 3: Data Processing with PySpark\n",
    "Create a data_processor.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b094045-3d87-4b06-951e-e16e10267639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from utils import create_spark_session\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.spark = create_spark_session()\n",
    "        \n",
    "    def load_data(self, file_path, file_type=\"csv\", **options):\n",
    "        \"\"\"Load data from various sources\"\"\"\n",
    "        if file_type == \"csv\":\n",
    "            return self.spark.read.csv(file_path, **options)\n",
    "        elif file_type == \"json\":\n",
    "            return self.spark.read.json(file_path, **options)\n",
    "        elif file_type == \"parquet\":\n",
    "            return self.spark.read.parquet(file_path, **options)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "            \n",
    "    def basic_clean(self, df, text_column):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        from pyspark.sql.functions import trim, lower, regexp_replace\n",
    "        \n",
    "        return df.withColumn(text_column, trim(col(text_column))) \\\n",
    "                 .withColumn(text_column, lower(col(text_column))) \\\n",
    "                 .withColumn(text_column, regexp_replace(col(text_column), r\"[^\\w\\s]\", \"\"))\n",
    "    \n",
    "    def process_large_texts(self, df, text_column, chunk_size=512):\n",
    "        \"\"\"Chunk large texts for LLM processing\"\"\"\n",
    "        @udf(ArrayType(StringType()))\n",
    "        def chunk_text(text):\n",
    "            if not text:\n",
    "                return []\n",
    "            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "            \n",
    "        return df.withColumn(f\"{text_column}_chunks\", chunk_text(col(text_column)))\n",
    "    \n",
    "    def save_processed_data(self, df, output_path, output_format=\"parquet\"):\n",
    "        \"\"\"Save processed data\"\"\"\n",
    "        if output_format == \"parquet\":\n",
    "            df.write.parquet(output_path, mode=\"overwrite\")\n",
    "        elif output_format == \"csv\":\n",
    "            df.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output format: {output_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8240d4-04aa-4824-989e-b3a639b40b4e",
   "metadata": {},
   "source": [
    "# Step 4: LLM Integration\n",
    "Create llm_integration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633e3ca-6185-450c-9f29-51294d517f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "class LLMIntegration:\n",
    "    def __init__(self, local_model_path=None, openai_key=None):\n",
    "        self.local_llm = None\n",
    "        self.openai_client = None\n",
    "        \n",
    "        if local_model_path:\n",
    "            self._load_local_model(local_model_path)\n",
    "        if openai_key:\n",
    "            self._setup_openai(openai_key)\n",
    "    \n",
    "    def _load_local_model(self, model_path):\n",
    "        \"\"\"Load local Llama model\"\"\"\n",
    "        print(\"Loading local Llama model...\")\n",
    "        self.local_llm = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_path,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(\"Local model loaded successfully\")\n",
    "    \n",
    "    def _setup_openai(self, api_key):\n",
    "        \"\"\"Initialize OpenAI client\"\"\"\n",
    "        self.openai_client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def get_local_llm_udf(self, max_length=50):\n",
    "        \"\"\"Create PySpark UDF for local LLM\"\"\"\n",
    "        def generate_text(prompt):\n",
    "            if not self.local_llm or not prompt:\n",
    "                return \"\"\n",
    "            result = self.local_llm(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return result[0]['generated_text']\n",
    "        \n",
    "        return udf(generate_text, StringType())\n",
    "    \n",
    "    def get_openai_udf(self, model=\"gpt-3.5-turbo\", max_tokens=50):\n",
    "        \"\"\"Create PySpark UDF for OpenAI API\"\"\"\n",
    "        def generate_text(prompt):\n",
    "            if not self.openai_client or not prompt:\n",
    "                return \"\"\n",
    "            try:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"OpenAI API error: {e}\")\n",
    "                return \"\"\n",
    "        \n",
    "        return udf(generate_text, StringType())\n",
    "    \n",
    "    def batch_process_with_local_llm(self, spark_df, text_column, output_column=\"llm_output\"):\n",
    "        \"\"\"Batch process text column with local LLM\"\"\"\n",
    "        llm_udf = self.get_local_llm_udf()\n",
    "        return spark_df.withColumn(output_column, llm_udf(col(text_column)))\n",
    "    \n",
    "    def batch_process_with_openai(self, spark_df, text_column, output_column=\"openai_output\"):\n",
    "        \"\"\"Batch process text column with OpenAI\"\"\"\n",
    "        openai_udf = self.get_openai_udf()\n",
    "        return spark_df.withColumn(output_column, openai_udf(col(text_column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ebd98-6f0d-4e20-8586-c0b3d4fb9f42",
   "metadata": {},
   "source": [
    "# Step 5: Main Application\n",
    "Create main.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407422fb-6285-4a77-acec-6300cf423b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import DataProcessor\n",
    "from llm_integration import LLMIntegration\n",
    "from utils import create_spark_session, stop_spark_session\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Initialize\n",
    "    spark = create_spark_session(\"LLM-Spark-Project\")\n",
    "    processor = DataProcessor()\n",
    "    llm_integration = LLMIntegration(\n",
    "        local_model_path=\"path/to/your/llama/model\",  # Update this\n",
    "        openai_key=\"your_openai_key\"  # Optional, update if needed\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Load and process data\n",
    "        print(\"Loading and processing data...\")\n",
    "        df = processor.load_data(\"data/input_data.csv\", header=True, inferSchema=True)\n",
    "        cleaned_df = processor.basic_clean(df, \"text_column\")\n",
    "        chunked_df = processor.process_large_texts(cleaned_df, \"text_column\")\n",
    "        \n",
    "        # LLM Processing\n",
    "        print(\"Processing with local LLM...\")\n",
    "        start_time = time.time()\n",
    "        local_llm_df = llm_integration.batch_process_with_local_llm(chunked_df, \"text_column_chunks\")\n",
    "        print(f\"Local LLM processing took {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Optional OpenAI Processing\n",
    "        if llm_integration.openai_client:\n",
    "            print(\"Processing with OpenAI...\")\n",
    "            start_time = time.time()\n",
    "            openai_df = llm_integration.batch_process_with_openai(chunked_df, \"text_column_chunks\")\n",
    "            print(f\"OpenAI processing took {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Save results\n",
    "        processor.save_processed_data(local_llm_df, \"data/output/local_llm_results\")\n",
    "        if llm_integration.openai_client:\n",
    "            processor.save_processed_data(openai_df, \"data/output/openai_results\")\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    finally:\n",
    "        stop_spark_session(spark)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38163f6-4d4d-4282-9fbb-3f8b24182149",
   "metadata": {},
   "source": [
    "# Step 6: Jupyter Notebook Example\n",
    "Create a notebook LLM_Spark_Demo.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680168b-0df9-4b34-a4af-2f411649f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "from utils import create_spark_session\n",
    "from data_processor import DataProcessor\n",
    "from llm_integration import LLMIntegration\n",
    "\n",
    "spark = create_spark_session(\"LLM-Spark-Notebook\")\n",
    "processor = DataProcessor()\n",
    "llm = LLMIntegration(local_model_path=\"path/to/your/llama/model\")\n",
    "\n",
    "# Sample data processing\n",
    "sample_data = [(\"This is a sample text to process.\", 1),\n",
    "               (\"Another example of text data for analysis.\", 2)]\n",
    "               \n",
    "df = spark.createDataFrame(sample_data, [\"text\", \"id\"])\n",
    "display(df)\n",
    "\n",
    "# Process with local LLM\n",
    "processed_df = llm.batch_process_with_local_llm(df, \"text\")\n",
    "display(processed_df)\n",
    "\n",
    "# Stop Spark when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65c3fd-f795-479d-93fc-64504120d932",
   "metadata": {},
   "source": [
    "# Step 7: Performance Optimization\n",
    "Add to data_processor.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a03cc8-0208-4329-adff-e1b193c70dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_llm_processing(self, df, text_column, cache=True):\n",
    "    \"\"\"Optimize DataFrame for LLM processing\"\"\"\n",
    "    # Repartition based on text length for better load balancing\n",
    "    from pyspark.sql.functions import length\n",
    "    \n",
    "    df = df.withColumn(\"text_length\", length(col(text_column)))\n",
    "    df = df.repartition(\"text_length\")\n",
    "    \n",
    "    if cache:\n",
    "        df.cache()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parallel_llm_processing(self, df, processing_function, batch_size=100):\n",
    "    \"\"\"Process data in parallel batches\"\"\"\n",
    "    from pyspark.sql.functions import pandas_udf\n",
    "    import pandas as pd\n",
    "    \n",
    "    @pandas_udf(StringType())\n",
    "    def batch_process(texts: pd.Series) -> pd.Series:\n",
    "        return texts.apply(processing_function)\n",
    "    \n",
    "    return df.withColumn(\"processed_text\", batch_process(col(\"text_chunk\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e156d5e-18d8-4bac-9e2e-54ea8d6cfb5c",
   "metadata": {},
   "source": [
    "# Step 8: Error Handling and Logging\n",
    "Create logging_utils.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946797d-0a30-4926-b88d-5487ac4a6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging for both Python and Spark\"\"\"\n",
    "    # Python logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('logs/llm_spark.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Spark logging\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    log4jLogger = sc._jvm.org.apache.log4j\n",
    "    logger = log4jLogger.LogManager.getLogger(__name__)\n",
    "    logger.info(\"Spark logging initialized\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def log_errors(func):\n",
    "    \"\"\"Decorator for error logging\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger = setup_logging()\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {func.__name__}: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871b682-e7f6-4d34-84fb-c69801ac9ca7",
   "metadata": {},
   "source": [
    "# Project Structure Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b481c59-09e5-4b6d-985f-57898da9b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm-spark-project/\n",
    "│\n",
    "├── data/\n",
    "│   ├── input/            # Raw input data\n",
    "│   └── output/           # Processed output\n",
    "│\n",
    "├── logs/                 # Log files\n",
    "│\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── main.py           # Main application\n",
    "│   ├── data_processor.py # PySpark data processing\n",
    "│   ├── llm_integration.py # LLM integration\n",
    "│   ├── utils.py          # Spark utilities\n",
    "│   └── logging_utils.py  # Logging configuration\n",
    "│\n",
    "├── notebooks/            # Jupyter notebooks\n",
    "│   └── LLM_Spark_Demo.ipynb\n",
    "│\n",
    "├── requirements.txt      # Python dependencies\n",
    "│\n",
    "├── README.md            # Project documentation\n",
    "└── .gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7842fd-0da1-4db8-a3b3-e3fbe71543ac",
   "metadata": {},
   "source": [
    "# Additional Recommendations for My GitHub Project\n",
    "## Documentation: Have to create a detailed README.md explaining:\n",
    "\n",
    "## Project purpose\n",
    "\n",
    "## Setup instructions\n",
    "\n",
    "## Configuration options\n",
    "\n",
    "## Example use cases\n",
    "\n",
    "## Sample Datasets: Have to include small sample datasets to demonstrate functionality\n",
    "\n",
    "## Docker Support: Have to consider adding Dockerfile for easy reproducibility\n",
    "\n",
    "## Benchmarking: Have to add performance comparison between local LLM and OpenAI\n",
    "\n",
    "## Advanced Features:\n",
    "\n",
    "## Have to add support for other LLMs\n",
    "\n",
    "## Have to implement caching mechanisms\n",
    "\n",
    "## Have to add visualization capabilities for results\n",
    "\n",
    "## This architecture provides a robust foundation for my project that showcases distributed data processing with PySpark combined with LLM capabilities. The modular design makes it easy to extend and adapt to different use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fefde-fe45-4d81-a11c-b012a9285535",
   "metadata": {},
   "source": [
    "### The last modified timestamp **********29th March 2025*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7794a-f3dd-40bb-85b9-611566abb719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
